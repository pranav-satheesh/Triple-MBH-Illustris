{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py    \n",
    "import numpy as np    \n",
    "import pandas as pd\n",
    "from astropy.cosmology import WMAP9 as cosmo\n",
    "from astropy.cosmology import z_at_value\n",
    "import astropy.units as u\n",
    "from astropy import constants as const\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yrsec = (1*u.yr).to(u.s).value # 1 year in secs\n",
    "solar_mass = const.M_sun.value #in Kgs\n",
    "#hubble_time = (1/cosmo.H(0)).to(u.yr).value #Hubble time = 1/H0 \n",
    "age_of_the_universe = cosmo.age(0).to(u.yr).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parent folder\n",
    "pfolder = \"/Users/pranavsatheesh/Triples/Github/\"\n",
    "import sys\n",
    "sys.path.append(pfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary merger files \n",
    "file_name = pfolder + \"Illustris_Data/mbhb-evolution_no-ecc_lc-full-0.6.hdf5\"\n",
    "f1 = h5py.File(file_name,'r') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grp:  evolution\n",
      "/evolution\n",
      "key:  dadt\n",
      "desc :  Hardening rate of each binary at each timestep  Shape: binaries, steps.  Units: [cm/s]\n",
      "shape :  binaries, steps\n",
      "units :  cm/s\n",
      "key:  dadt_df\n",
      "desc :  Hardening rate due to dynamical friction of each binary at each timestep  Shape: binaries, steps.  Units: [cm/s]\n",
      "shape :  binaries, steps\n",
      "units :  cm/s\n",
      "key:  dadt_gw\n",
      "desc :  Hardening rate due to gravitational waves of each binary at each timestep  Shape: binaries, steps.  Units: [cm/s]\n",
      "shape :  binaries, steps\n",
      "units :  cm/s\n",
      "key:  dadt_lc\n",
      "desc :  Hardening rate due to stellar-scattering of each binary at each timestep  Shape: binaries, steps.  Units: [cm/s]\n",
      "shape :  binaries, steps\n",
      "units :  cm/s\n",
      "key:  dadt_vd\n",
      "desc :  Hardening rate due to disk torque of each binary at each timestep  Shape: binaries, steps.  Units: [cm/s]\n",
      "shape :  binaries, steps\n",
      "units :  cm/s\n",
      "key:  dedt\n",
      "desc :  Eccentricity evolution rate of each binary at each timestep  Shape: binaries, steps.  Units: [1/s]\n",
      "shape :  binaries, steps\n",
      "units :  1/s\n",
      "key:  eccen\n",
      "desc :  Eccentricity of each binary at each timestep  Shape: binaries, steps.  Units: [-]\n",
      "shape :  binaries, steps\n",
      "units :  -\n",
      "key:  masses\n",
      "desc :  Component masses of each binary at the time of formation (illustris \"merger\").  Shape: binaries, blackholes.  Units: [grams]\n",
      "shape :  binaries, blackholes\n",
      "units :  grams\n",
      "key:  mdot_eff\n",
      "desc :  Accretion rate onto the binary (from the Illustris single-MBH (remnant) accretion rates).  Shape: binaries, steps.  Units: [g/s]\n",
      "shape :  binaries, steps\n",
      "units :  g/s\n",
      "key:  scales\n",
      "desc :  Scale-factor (\"a\") of the universe for each binary at each timestep.  Shape: binaries, steps.  Units: [-]\n",
      "shape :  binaries, steps\n",
      "units :  -\n",
      "key:  sep\n",
      "desc :  Binary separation for each binary at each timestep  Shape: binaries, steps.  Units: [cm]\n",
      "shape :  binaries, steps\n",
      "units :  cm\n",
      "key:  times\n",
      "desc :  Age of the universe for each binary at each timestep.  Shape: binaries, steps.  Units: [seconds]\n",
      "shape :  binaries, steps\n",
      "units :  seconds\n",
      "key:  val_inds\n",
      "desc :  index of valid binaries.  Shape: indices.  Units: [NA]\n",
      "shape :  indices\n",
      "units :  NA\n",
      "grp:  meta\n",
      "/meta\n",
      "key:  SubhaloMassInHalfRadType\n",
      "desc :  Mass in each subhalo (within the stellar half-mass radius), by particle-type, in the snapshot following the 'merger' (from Illustris)  Shape: binaries, ptypes.  Units: [grams]\n",
      "shape :  binaries, ptypes\n",
      "units :  grams\n",
      "key:  SubhaloSFRinHalfRad\n",
      "desc :  SFR within the subhalo stellar half-mass radius, in the snapshot following the 'merger' (from Illustris)  Shape: binaries,.  Units: [Msol/yr]\n",
      "shape :  binaries,\n",
      "units :  Msol/yr\n",
      "key:  snapshot\n",
      "desc :  Snapshot following the binary 'merger' (from Illustris).  Shape: binaries,.  Units: [-]\n",
      "shape :  binaries,\n",
      "units :  -\n",
      "key:  subhalo_id\n",
      "desc :  ID Number for the Subhalo containing the binary, in the snapshot following the 'merger' (from Illustris).  Shape: binaries,.  Units: [-]\n",
      "shape :  binaries,\n",
      "units :  -\n"
     ]
    }
   ],
   "source": [
    "for grp in f1.keys():\n",
    "    print(\"grp: \",grp)\n",
    "    print(f1[grp].name)\n",
    "    for a in f1[grp].attrs.keys():\n",
    "        print(a)\n",
    "    for k in f1[grp].keys():\n",
    "        print(\"key: \", k)\n",
    "        for a in f1[grp][k].attrs:\n",
    "            print(a,\": \",f1[grp][k].attrs[a])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary merger files \n",
    "file_name = pfolder + \"Illustris_Data/mbhb-evolution_no-ecc_lc-full-0.6.hdf5\"\n",
    "f1 = h5py.File(file_name,'r') \n",
    "\n",
    "Ms = np.array(f1['evolution']['masses'])\n",
    "t = np.array(f1['evolution']['times'])\n",
    "r = np.array(f1['evolution']['sep'])\n",
    "\n",
    "Ms = Ms/(solar_mass*10**3)\n",
    "Nbinary = len(Ms)\n",
    "M1 = Ms[:,0]\n",
    "M2 = Ms[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Unable to open object (object 'envs' doesn't exist)\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(file_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 2\u001b[0m             envs \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43menvs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      3\u001b[0m             envs_in \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menvs_in\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/ast/lib/python3.10/site-packages/h5py/_hl/group.py:328\u001b[0m, in \u001b[0;36mGroup.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid HDF5 object reference\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m)):\n\u001b[0;32m--> 328\u001b[0m     oid \u001b[38;5;241m=\u001b[39m \u001b[43mh5o\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessing a group is done with bytes or str, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(name)))\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5o.pyx:190\u001b[0m, in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Unable to open object (object 'envs' doesn't exist)\""
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "Share\n",
    "\n",
    "\n",
    "You said:\n",
    "In npz files can I store multidimensional arrays too ?\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "Yes, you can store multidimensional arrays in .npz files. An .npz file is a compressed archive of multiple numpy arrays stored in .npy format. You can save and load multidimensional arrays (such as 2D, 3D, or higher-dimensional arrays) using numpy functions like np.savez or np.savez_compressed and load them with np.load.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "Saving a multidimensional array:\n",
    "python\n",
    "Copy code\n",
    "import numpy as np\n",
    "\n",
    "# Creating a 3D array\n",
    "arr = np.random.random((4, 3, 2))\n",
    "\n",
    "# Saving it to an npz file\n",
    "np.savez('multidimensional_array.npz', array_name=arr)\n",
    "Loading the multidimensional array:\n",
    "python\n",
    "Copy code\n",
    "loaded_data = np.load('multidimensional_array.npz')\n",
    "loaded_array = loaded_data['array_name']\n",
    "\n",
    "print(loaded_array)\n",
    "You can store multiple arrays with different names in the same .npz file as well.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Which is better to save some object attributes (arrays) in file ?\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "The best method to save object attributes, especially if they are arrays (e.g., numpy arrays), depends on your specific needs for file size, readability, portability, and whether you need to save complex objects. Here are some common options:\n",
    "\n",
    "1. .npz or .npy files (using Numpy)\n",
    "Use case: Best for storing and loading numpy arrays quickly and efficiently.\n",
    "Pros:\n",
    "Handles large, multidimensional arrays well.\n",
    "Easy to use if you're dealing primarily with numerical data.\n",
    ".npz allows storing multiple arrays in one file.\n",
    "Cons:\n",
    "Not human-readable (binary format).\n",
    "Limited to storing numpy arrays (other object types require conversion).\n",
    "python\n",
    "Copy code\n",
    "# Saving arrays\n",
    "np.savez('attributes.npz', attr1=obj.attr1, attr2=obj.attr2)\n",
    "\n",
    "# Loading arrays\n",
    "data = np.load('attributes.npz')\n",
    "attr1 = data['attr1']\n",
    "attr2 = data['attr2']\n",
    "2. HDF5 format (using h5py)\n",
    "Use case: Ideal for large datasets, multidimensional arrays, and hierarchical data.\n",
    "Pros:\n",
    "Efficient for large arrays and hierarchical data.\n",
    "Allows partial loading of datasets (e.g., if the array is too large to fit into memory).\n",
    "Cons:\n",
    "Requires the h5py library.\n",
    "Not as portable as simple text formats.\n",
    "python\n",
    "Copy code\n",
    "import h5py\n",
    "\n",
    "# Saving arrays\n",
    "with h5py.File('attributes.h5', 'w') as f:\n",
    "    f.create_dataset('attr1', data=obj.attr1)\n",
    "    f.create_dataset('attr2', data=obj.attr2)\n",
    "\n",
    "# Loading arrays\n",
    "with h5py.File('attributes.h5', 'r') as f:\n",
    "    attr1 = f['attr1'][:]\n",
    "    attr2 = f['attr2'][:]\n",
    "3. Pickle (using pickle)\n",
    "Use case: For saving entire Python objects, including arrays, lists, dictionaries, and custom object attributes.\n",
    "Pros:\n",
    "Saves the entire Python object with all attributes, not just arrays.\n",
    "Easy to use for complex objects with many attributes.\n",
    "Cons:\n",
    "Binary format (not human-readable).\n",
    "Can be insecure when loading untrusted files (avoid loading pickle files from unknown sources).\n",
    "python\n",
    "Copy code\n",
    "import pickle\n",
    "\n",
    "# Saving the object\n",
    "with open('object.pkl', 'wb') as f:\n",
    "    pickle.dump(obj, f)\n",
    "\n",
    "# Loading the object\n",
    "with open('object.pkl', 'rb') as f:\n",
    "    obj = pickle.load(f)\n",
    "4. JSON (if arrays are small and simple)\n",
    "Use case: Best for human-readable and portable storage, but limited to basic data types (e.g., lists and dictionaries).\n",
    "Pros:\n",
    "Human-readable and easy to share across platforms.\n",
    "Supported by many languages.\n",
    "Cons:\n",
    "Doesn’t handle numpy arrays directly (need to convert to lists first).\n",
    "Less efficient for large numerical data compared to .npz or HDF5.\n",
    "python\n",
    "Copy code\n",
    "import json\n",
    "\n",
    "# Converting arrays to lists and saving as JSON\n",
    "with open('attributes.json', 'w') as f:\n",
    "    json.dump({'attr1': obj.attr1.tolist(), 'attr2': obj.attr2.tolist()}, f)\n",
    "\n",
    "# Loading from JSON\n",
    "with open('attributes.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    attr1 = np.array(data['attr1'])\n",
    "    attr2 = np.array(data['attr2'])\n",
    "Which is better?\n",
    "For numpy arrays: .npz or HDF5 are the best options. They are efficient, simple to use with numpy arrays, and store large data well.\n",
    "For complex objects with attributes: pickle is the easiest, as it can store the entire object directly.\n",
    "For portability and human-readability: JSON can be useful, but it’s limited for numerical data.\n",
    "If you're only saving numpy arrays as object attributes, go with .npz or HDF5, depending on the size and complexity of your data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "\n",
    "    def isolated_binaries_data(self,save_file):\n",
    "\n",
    "\n",
    "        '''This function is for extracting all the info about isolated binaries and store them in\n",
    "        a csv file'''\n",
    "        masses_binary = self.masses[self.binary_mask]\n",
    "        masses_binary = np.sort(masses_binary)\n",
    "        M1_binary = masses_binary[:,1] #bigger one is primary\n",
    "        M2_binary = masses_binary[:,0]\n",
    "        qbh_inner = M2_binary/M1_binary\n",
    "        mbh_t_form = self.sim_tmrg[self.binary_mask]\n",
    "\n",
    "        t_evol_binary = self._next.evol_tmrg[self.binary_mask]\n",
    "        #evolution time of binary in inspiral\n",
    "\n",
    "\n",
    "        #gas-fraction, Escape speed and BHIDs\n",
    "        fgas,Vescape,Phi_DM,Phi_star,M_halo,M_star,rhalf,sigma,rbulge = self.host_galaxy_properties()\n",
    "        fgas = fgas[self.binary_mask]\n",
    "        Vescape = Vescape[self.binary_mask]\n",
    "        binary_ids = self.merger_ids[self.binary_mask]\n",
    "        Phi_DM = Phi_DM[self.binary_mask]\n",
    "        Phi_star = Phi_star[self.binary_mask]\n",
    "        M_halo = M_halo[self.binary_mask]\n",
    "        M_star = M_star[self.binary_mask]\n",
    "        rhalf = rhalf[self.binary_mask]\n",
    "        sigma = sigma[self.binary_mask]\n",
    "        rbulge = rbulge[self.binary_mask]\n",
    "\n",
    "        age_of_the_universe = cosmo.age(0).to(u.yr).value\n",
    "        merged_before_z0_flag = []\n",
    "        z_binary = []\n",
    "    \n",
    "        for time in t_evol_binary:\n",
    "            if time >= age_of_the_universe:\n",
    "                merged_before_z0_flag.append(\"No\")\n",
    "                z_binary.append(0)\n",
    "            else:\n",
    "                merged_before_z0_flag.append(\"Yes\")\n",
    "                z_binary.append(z_at_value(cosmo.age,(time/10**9)*u.Gyr,zmin=1e-13).value)\n",
    "\n",
    "        #z_binary = z_at_value(cosmo.age,(t_evol_binary/10**9)*u.Gyr,zmin=1e-10)\n",
    "\n",
    "        df = pd.DataFrame([M1_binary,M2_binary,qbh_inner,t_evol_binary,z_binary,mbh_t_form,merged_before_z0_flag,binary_ids[:,0],binary_ids[:,1],fgas,Vescape,Phi_DM,Phi_star,M_halo,M_star,rhalf,sigma,rbulge])\n",
    "        df = df.transpose()\n",
    "        df.columns = ['M1','M2','qin','t_merger','z_merger','t_form','merger_flag','bhid1','bhid2','f-gas','Vescape','Phi_DM','Phi_star','M_halo','M_star','rhalf','sigma','rbulge']\n",
    "        df.to_csv(save_file+'iso_binaries_data_from_ill.csv',index=False)\n",
    "        print(\"File saved at\",save_file+\"iso_binaries_data_from_ill.csv\")\n",
    "\n",
    "        return None\n",
    "\n",
    "Can you rewrite this to store in hdf5 instead pf pandas\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "Sure! Here's the rewritten version of your code that stores the extracted information in an HDF5 file instead of using a CSV with pandas. The h5py library is used to handle the HDF5 file.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import numpy as np\n",
    "import h5py\n",
    "from astropy.cosmology import z_at_value\n",
    "from astropy import units as u\n",
    "\n",
    "def isolated_binaries_data(self, save_file):\n",
    "    '''This function extracts all the info about isolated binaries and stores them in an HDF5 file'''\n",
    "\n",
    "    # Extracting binary masses and sorting them\n",
    "    masses_binary = self.masses[self.binary_mask]\n",
    "    masses_binary = np.sort(masses_binary)\n",
    "    M1_binary = masses_binary[:, 1]  # Bigger one is primary\n",
    "    M2_binary = masses_binary[:, 0]\n",
    "    qbh_inner = M2_binary / M1_binary\n",
    "    mbh_t_form = self.sim_tmrg[self.binary_mask]\n",
    "\n",
    "    # Evolution time of binary in inspiral\n",
    "    t_evol_binary = self._next.evol_tmrg[self.binary_mask]\n",
    "\n",
    "    # Extract gas-fraction, escape speed, and other host galaxy properties\n",
    "    fgas, Vescape, Phi_DM, Phi_star, M_halo, M_star, rhalf, sigma, rbulge = self.host_galaxy_properties()\n",
    "    fgas = fgas[self.binary_mask]\n",
    "    Vescape = Vescape[self.binary_mask]\n",
    "    binary_ids = self.merger_ids[self.binary_mask]\n",
    "    Phi_DM = Phi_DM[self.binary_mask]\n",
    "    Phi_star = Phi_star[self.binary_mask]\n",
    "    M_halo = M_halo[self.binary_mask]\n",
    "    M_star = M_star[self.binary_mask]\n",
    "    rhalf = rhalf[self.binary_mask]\n",
    "    sigma = sigma[self.binary_mask]\n",
    "    rbulge = rbulge[self.binary_mask]\n",
    "\n",
    "    # Computing merger flag and redshift\n",
    "    age_of_the_universe = cosmo.age(0).to(u.yr).value\n",
    "    merged_before_z0_flag = []\n",
    "    z_binary = []\n",
    "\n",
    "    for time in t_evol_binary:\n",
    "        if time >= age_of_the_universe:\n",
    "            merged_before_z0_flag.append(0)  # No\n",
    "            z_binary.append(0)\n",
    "        else:\n",
    "            merged_before_z0_flag.append(1)  # Yes\n",
    "            z_binary.append(z_at_value(cosmo.age, (time / 10**9) * u.Gyr, zmin=1e-13).value)\n",
    "\n",
    "    # Saving data to an HDF5 file\n",
    "    with h5py.File(save_file + 'iso_binaries_data_from_ill.h5', 'w') as hf:\n",
    "        hf.create_dataset('M1', data=M1_binary)\n",
    "        hf.create_dataset('M2', data=M2_binary)\n",
    "        hf.create_dataset('qin', data=qbh_inner)\n",
    "        hf.create_dataset('t_merger', data=t_evol_binary)\n",
    "        hf.create_dataset('z_merger', data=z_binary)\n",
    "        hf.create_dataset('t_form', data=mbh_t_form)\n",
    "        hf.create_dataset('merger_flag', data=merged_before_z0_flag)\n",
    "        hf.create_dataset('bhid1', data=binary_ids[:, 0])\n",
    "        hf.create_dataset('bhid2', data=binary_ids[:, 1])\n",
    "        hf.create_dataset('f-gas', data=fgas)\n",
    "        hf.create_dataset('Vescape', data=Vescape)\n",
    "        hf.create_dataset('Phi_DM', data=Phi_DM)\n",
    "        hf.create_dataset('Phi_star', data=Phi_star)\n",
    "        hf.create_dataset('M_halo', data=M_halo)\n",
    "        hf.create_dataset('M_star', data=M_star)\n",
    "        hf.create_dataset('rhalf', data=rhalf)\n",
    "        hf.create_dataset('sigma', data=sigma)\n",
    "        hf.create_dataset('rbulge', data=rbulge)\n",
    "\n",
    "    print(\"File saved at\", save_file + \"iso_binaries_data_from_ill.h5\")\n",
    "\n",
    "    return None\n",
    "Key Changes:\n",
    "Replaced the pandas DataFrame with h5py datasets.\n",
    "Used hf.create_dataset() for each attribute (array) you want to store in the HDF5 file.\n",
    "The merged flag (merged_before_z0_flag) is stored as a binary flag (0 for \"No\" and 1 for \"Yes\").\n",
    "This format is efficient for large datasets, allowing you to store all arrays hierarchically in the HDF5 file.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Is there an easier way ?\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "Yes, an easier and more convenient way to store object attributes (like arrays) in an HDF5 file is to use the h5py library's ability to store dictionaries or even the higher-level pandas interface, which works well with HDF5.\n",
    "\n",
    "However, if you want to avoid manually creating each dataset, you can streamline the process by writing a loop or saving the attributes as a group in HDF5. Here's an approach that simplifies things:\n",
    "\n",
    "Approach 1: Using a loop with h5py\n",
    "Instead of manually creating each dataset, you can place the arrays into a dictionary and loop through it to store the data in an HDF5 file. This reduces repetition in the code.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import numpy as np\n",
    "import h5py\n",
    "from astropy.cosmology import z_at_value\n",
    "from astropy import units as u\n",
    "\n",
    "def isolated_binaries_data(self, save_file):\n",
    "    '''This function extracts all the info about isolated binaries and stores them in an HDF5 file'''\n",
    "\n",
    "    # Extracting binary masses and sorting them\n",
    "    masses_binary = self.masses[self.binary_mask]\n",
    "    masses_binary = np.sort(masses_binary)\n",
    "    M1_binary = masses_binary[:, 1]  # Bigger one is primary\n",
    "    M2_binary = masses_binary[:, 0]\n",
    "    qbh_inner = M2_binary / M1_binary\n",
    "    mbh_t_form = self.sim_tmrg[self.binary_mask]\n",
    "\n",
    "    # Evolution time of binary in inspiral\n",
    "    t_evol_binary = self._next.evol_tmrg[self.binary_mask]\n",
    "\n",
    "    # Extract gas-fraction, escape speed, and other host galaxy properties\n",
    "    fgas, Vescape, Phi_DM, Phi_star, M_halo, M_star, rhalf, sigma, rbulge = self.host_galaxy_properties()\n",
    "    binary_ids = self.merger_ids[self.binary_mask]\n",
    "\n",
    "    # Filter the values by the binary mask\n",
    "    host_galaxy_data = {\n",
    "        \"fgas\": fgas[self.binary_mask],\n",
    "        \"Vescape\": Vescape[self.binary_mask],\n",
    "        \"Phi_DM\": Phi_DM[self.binary_mask],\n",
    "        \"Phi_star\": Phi_star[self.binary_mask],\n",
    "        \"M_halo\": M_halo[self.binary_mask],\n",
    "        \"M_star\": M_star[self.binary_mask],\n",
    "        \"rhalf\": rhalf[self.binary_mask],\n",
    "        \"sigma\": sigma[self.binary_mask],\n",
    "        \"rbulge\": rbulge[self.binary_mask]\n",
    "    }\n",
    "\n",
    "    # Computing merger flag and redshift\n",
    "    age_of_the_universe = cosmo.age(0).to(u.yr).value\n",
    "    merged_before_z0_flag = []\n",
    "    z_binary = []\n",
    "\n",
    "    for time in t_evol_binary:\n",
    "        if time >= age_of_the_universe:\n",
    "            merged_before_z0_flag.append(0)  # No\n",
    "            z_binary.append(0)\n",
    "        else:\n",
    "            merged_before_z0_flag.append(1)  # Yes\n",
    "            z_binary.append(z_at_value(cosmo.age, (time / 10**9) * u.Gyr, zmin=1e-13).value)\n",
    "\n",
    "    # Prepare data to store in a dictionary\n",
    "    data_to_save = {\n",
    "        'M1': M1_binary,\n",
    "        'M2': M2_binary,\n",
    "        'qin': qbh_inner,\n",
    "        't_merger': t_evol_binary,\n",
    "        'z_merger': z_binary,\n",
    "        't_form': mbh_t_form,\n",
    "        'merger_flag': merged_before_z0_flag,\n",
    "        'bhid1': binary_ids[:, 0],\n",
    "        'bhid2': binary_ids[:, 1],\n",
    "        **host_galaxy_data  # unpack host galaxy properties into the dictionary\n",
    "    }\n",
    "\n",
    "    # Save all data using a loop\n",
    "    with h5py.File(save_file + 'iso_binaries_data_from_ill.h5', 'w') as hf:\n",
    "        for key, value in data_to_save.items():\n",
    "            hf.create_dataset(key, data=value)\n",
    "\n",
    "    print(\"File saved at\", save_file + \"iso_binaries_data_from_ill.h5\")\n",
    "    \n",
    "    return None\n",
    "Key Improvements:\n",
    "Dictionary-based storage: We collect all the attributes into a data_to_save dictionary, which simplifies the process.\n",
    "Loop: Instead of manually creating datasets for each attribute, we loop over the dictionary and create datasets in one go.\n",
    "Dynamic inclusion: You can easily add or remove attributes from the dictionary without changing much of the code.\n",
    "Approach 2: Using pandas with HDF5 (Optional)\n",
    "If you're comfortable with using pandas, it can directly save the dataframe in HDF5 format, which is easy and more human-readable. This approach avoids h5py if you’re okay with pandas handling the HDF5 file.\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from astropy.cosmology import z_at_value\n",
    "from astropy import units as u\n",
    "\n",
    "def isolated_binaries_data(self, save_file):\n",
    "    '''This function extracts all the info about isolated binaries and stores them in an HDF5 file'''\n",
    "    \n",
    "    masses_binary = self.masses[self.binary_mask]\n",
    "    masses_binary = np.sort(masses_binary)\n",
    "    M1_binary = masses_binary[:, 1]\n",
    "    M2_binary = masses_binary[:, 0]\n",
    "    qbh_inner = M2_binary / M1_binary\n",
    "    mbh_t_form = self.sim_tmrg[self.binary_mask]\n",
    "    t_evol_binary = self._next.evol_tmrg[self.binary_mask]\n",
    "\n",
    "    fgas, Vescape, Phi_DM, Phi_star, M_halo, M_star, rhalf, sigma, rbulge = self.host_galaxy_properties()\n",
    "    fgas = fgas[self.binary_mask]\n",
    "    Vescape = Vescape[self.binary_mask]\n",
    "    binary_ids = self.merger_ids[self.binary_mask]\n",
    "    Phi_DM = Phi_DM[self.binary_mask]\n",
    "    Phi_star = Phi_star[self.binary_mask]\n",
    "    M_halo = M_halo[self.binary_mask]\n",
    "    M_star = M_star[self.binary_mask]\n",
    "    rhalf = rhalf[self.binary_mask]\n",
    "    sigma = sigma[self.binary_mask]\n",
    "    rbulge = rbulge[self.binary_mask]\n",
    "\n",
    "    age_of_the_universe = cosmo.age(0).to(u.yr).value\n",
    "    merged_before_z0_flag = []\n",
    "    z_binary = []\n",
    "\n",
    "    for time in t_evol_binary:\n",
    "        if time >= age_of_the_universe:\n",
    "            merged_before_z0_flag.append(\"No\")\n",
    "            z_binary.append(0)\n",
    "        else:\n",
    "            merged_before_z0_flag.append(\"Yes\")\n",
    "            z_binary.append(z_at_value(cosmo.age, (time / 10**9) * u.Gyr, zmin=1e-13).value)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'M1': M1_binary,\n",
    "        'M2': M2_binary,\n",
    "        'qin': qbh_inner,\n",
    "        't_merger': t_evol_binary,\n",
    "        'z_merger': z_binary,\n",
    "        't_form': mbh_t_form,\n",
    "        'merger_flag': merged_before_z0_flag,\n",
    "        'bhid1': binary_ids[:, 0],\n",
    "        'bhid2': binary_ids[:, 1],\n",
    "        'f-gas': fgas,\n",
    "        'Vescape': Vescape,\n",
    "        'Phi_DM': Phi_DM,\n",
    "        'Phi_star': Phi_star,\n",
    "        'M_halo': M_halo,\n",
    "        'M_star': M_star,\n",
    "        'rhalf': rhalf,\n",
    "        'sigma': sigma,\n",
    "        'rbulge': rbulge\n",
    "    })\n",
    "\n",
    "    # Save DataFrame to HDF5\n",
    "    df.to_hdf(save_file + 'iso_binaries_data_from_ill.h5', key='iso_binaries', mode='w')\n",
    "\n",
    "    print(\"File saved at\", save_file + \"iso_binaries_data_from_ill.h5\")\n",
    "    \n",
    "    return None\n",
    "Which Approach is Easier?\n",
    "If you're comfortable with h5py and want more control over your data storage structure, Approach 1 with a loop is simpler and flexible.\n",
    "If you prefer simplicity and are already using pandas, Approach 2 (pandas + HDF5) is the easiest, as it handles all the complexity for you while still using the HDF5 format.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "Now I have a ndimensional array \n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "For multidimensional arrays, you can easily store them in both h5py and pandas with HDF5. Here's how you can adapt both approaches to handle an n-dimensional array.\n",
    "\n",
    "Storing n-dimensional Arrays with h5py\n",
    "In h5py, you can store any numpy array, including n-dimensional arrays, just as you would with 1D or 2D arrays. You don't need to change much from the previous code; h5py.create_dataset() naturally supports n-dimensional data.\n",
    "\n",
    "Example for h5py:\n",
    "python\n",
    "Copy code\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "def save_nd_array_to_hdf5(nd_array, save_file):\n",
    "    '''Function to save an n-dimensional array into an HDF5 file'''\n",
    "\n",
    "    # Save the n-dimensional array\n",
    "    with h5py.File(save_file + 'nd_array_data.h5', 'w') as hf:\n",
    "        hf.create_dataset('nd_array', data=nd_array)\n",
    "\n",
    "    print(\"N-dimensional array saved at\", save_file + \"nd_array_data.h5\")\n",
    "\n",
    "# Example usage\n",
    "nd_array = np.random.rand(5, 4, 3)  # 3D array as an example\n",
    "save_nd_array_to_hdf5(nd_array, \"path_to_save/\")\n",
    "In this code:\n",
    "\n",
    "The array nd_array is saved as a dataset in the HDF5 file.\n",
    "You can replace nd_array with any n-dimensional array (3D, 4D, etc.), and h5py will handle it automatically.\n",
    "Storing n-dimensional Arrays with pandas + HDF5\n",
    "Pandas is great for 1D and 2D data, but if you need to store an n-dimensional array, it's best to stick with h5py directly since pandas doesn't natively support higher-dimensional structures beyond DataFrames.\n",
    "\n",
    "However, if you're determined to use pandas, you could flatten your n-dimensional array into 2D (or reshape it) and then store it in HDF5. Here’s how you could do that:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def save_nd_array_with_pandas(nd_array, save_file):\n",
    "    '''Function to save an n-dimensional array into HDF5 using pandas'''\n",
    "\n",
    "    # Reshape the array into 2D (flatten if necessary)\n",
    "    reshaped_array = nd_array.reshape(nd_array.shape[0], -1)\n",
    "    \n",
    "    # Create a DataFrame from the reshaped array\n",
    "    df = pd.DataFrame(reshaped_array)\n",
    "\n",
    "    # Save DataFrame to HDF5\n",
    "    df.to_hdf(save_file + 'nd_array_data.h5', key='nd_array', mode='\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "in the previous function\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "To integrate n-dimensional array handling into your isolated_binaries_data function while saving the data in HDF5 format, you can follow these steps. Here’s the modified version that accommodates n-dimensional arrays:\n",
    "\n",
    "Updated Function to Handle n-Dimensional Arrays\n",
    "Assuming you want to include n-dimensional arrays among the attributes being saved, here’s how you can do it with h5py:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import numpy as np\n",
    "import h5py\n",
    "from astropy.cosmology import z_at_value\n",
    "from astropy import units as u\n",
    "\n",
    "def isolated_binaries_data(self, save_file):\n",
    "    '''This function extracts all the info about isolated binaries and stores them in an HDF5 file'''\n",
    "\n",
    "    # Extracting binary masses and sorting them\n",
    "    masses_binary = self.masses[self.binary_mask]\n",
    "    masses_binary = np.sort(masses_binary)\n",
    "    M1_binary = masses_binary[:, 1]  # Bigger one is primary\n",
    "    M2_binary = masses_binary[:, 0]\n",
    "    qbh_inner = M2_binary / M1_binary\n",
    "    mbh_t_form = self.sim_tmrg[self.binary_mask]\n",
    "\n",
    "    # Evolution time of binary in inspiral\n",
    "    t_evol_binary = self._next.evol_tmrg[self.binary_mask]\n",
    "\n",
    "    # Extract gas-fraction, escape speed, and other host galaxy properties\n",
    "    fgas, Vescape, Phi_DM, Phi_star, M_halo, M_star, rhalf, sigma, rbulge = self.host_galaxy_properties()\n",
    "    binary_ids = self.merger_ids[self.binary_mask]\n",
    "\n",
    "    # Filter the values by the binary mask\n",
    "    host_galaxy_data = {\n",
    "        \"fgas\": fgas[self.binary_mask],\n",
    "        \"Vescape\": Vescape[self.binary_mask],\n",
    "        \"Phi_DM\": Phi_DM[self.binary_mask],\n",
    "        \"Phi_star\": Phi_star[self.binary_mask],\n",
    "        \"M_halo\": M_halo[self.binary_mask],\n",
    "        \"M_star\": M_star[self.binary_mask],\n",
    "        \"rhalf\": rhalf[self.binary_mask],\n",
    "        \"sigma\": sigma[self.binary_mask],\n",
    "        \"rbulge\": rbulge[self.binary_mask]\n",
    "    }\n",
    "\n",
    "    # Assuming you have an n-dimensional array to save\n",
    "    n_dimensional_array = self.get_some_nd_array()  # Replace with your actual method to get the nD array\n",
    "\n",
    "    # Computing merger flag and redshift\n",
    "    age_of_the_universe = cosmo.age(0).to(u.yr).value\n",
    "    merged_before_z0_flag = []\n",
    "    z_binary = []\n",
    "\n",
    "    for time in t_evol_binary:\n",
    "        if time >= age_of_the_universe:\n",
    "            merged_before_z0_flag.append(0)  # No\n",
    "            z_binary.append(0)\n",
    "        else:\n",
    "            merged_before_z0_flag.append(1)  # Yes\n",
    "            z_binary.append(z_at_value(cosmo.age, (time / 10**9) * u.Gyr, zmin=1e-13).value)\n",
    "\n",
    "    # Prepare data to store in a dictionary\n",
    "    data_to_save = {\n",
    "        'M1': M1_binary,\n",
    "        'M2': M2_binary,\n",
    "        'qin': qbh_inner,\n",
    "        't_merger': t_evol_binary,\n",
    "        'z_merger': z_binary,\n",
    "        't_form': mbh_t_form,\n",
    "        'merger_flag': merged_before_z0_flag,\n",
    "        'bhid1': binary_ids[:, 0],\n",
    "        'bhid2': binary_ids[:, 1],\n",
    "        **host_galaxy_data,  # Unpack host galaxy properties into the dictionary\n",
    "        'n_dimensional_data': n_dimensional_array  # Add your n-dimensional array here\n",
    "    }\n",
    "\n",
    "    # Save all data using a loop\n",
    "    with h5py.File(save_file + 'iso_binaries_data_from_ill.h5', 'w') as hf:\n",
    "        for key, value in data_to_save.items():\n",
    "            hf.create_dataset(key, data=value)\n",
    "\n",
    "    print(\"File saved at\", save_file + \"iso_binaries_data_from_ill.h5\")\n",
    "\n",
    "    return None\n",
    "Key Changes:\n",
    "N-Dimensional Array: I added a placeholder method self.get_some_nd_array() that you should replace with your actual logic to retrieve the n-dimensional array you want to save.\n",
    "Inclusion in Data to Save: The n-dimensional array is included in the data_to_save dictionary, just like the other attributes.\n",
    "Saving the Data: All attributes, including the n-dimensional array, are saved using the loop that iterates over the dictionary.\n",
    "Storing n-Dimensional Data:\n",
    "The h5py.create_dataset() method handles n-dimensional data automatically, so you don't need to change anything else in the way you save datasets. Just ensure that the array you are passing to it is in the desired shape.\n",
    "\n",
    "This will save all your attributes, including the n-dimensional array, into a single HDF5 file neatly organized with keys for each dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "what is **host_galaxy_data and how will I read that ?\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "The **host_galaxy_data syntax in Python is used to unpack a dictionary into a function call or another dictionary. In this context, host_galaxy_data is a dictionary containing various properties related to the host galaxy, such as gas fraction, escape speed, and other attributes.\n",
    "\n",
    "Example Structure of host_galaxy_data\n",
    "Here’s how host_galaxy_data is structured in your code:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "host_galaxy_data = {\n",
    "    \"fgas\": fgas[self.binary_mask],\n",
    "    \"Vescape\": Vescape[self.binary_mask],\n",
    "    \"Phi_DM\": Phi_DM[self.binary_mask],\n",
    "    \"Phi_star\": Phi_star[self.binary_mask],\n",
    "    \"M_halo\": M_halo[self.binary_mask],\n",
    "    \"M_star\": M_star[self.binary_mask],\n",
    "    \"rhalf\": rhalf[self.binary_mask],\n",
    "    \"sigma\": sigma[self.binary_mask],\n",
    "    \"rbulge\": rbulge[self.binary_mask]\n",
    "}\n",
    "Reading the Data from HDF5\n",
    "To read back the data you saved in the HDF5 file, including the host_galaxy_data, you can use the following approach with h5py:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import h5py\n",
    "\n",
    "def read_hdf5_data(file_path):\n",
    "    '''Function to read data from the HDF5 file and return it as a dictionary'''\n",
    "\n",
    "    data = {}\n",
    "    \n",
    "    with h5py.File(file_path, 'r') as hf:\n",
    "        # Loop through each dataset in the file\n",
    "        for key in hf.keys():\n",
    "            data[key] = hf[key][...]  # Read the data into a NumPy array\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "file_path = 'path_to_save/iso_binaries_data_from_ill.h5'\n",
    "data = read_hdf5_data(file_path)\n",
    "\n",
    "# Accessing host galaxy data\n",
    "fgas = data['fgas']\n",
    "Vescape = data['Vescape']\n",
    "Phi_DM = data['Phi_DM']\n",
    "# ... and so on for other attributes\n",
    "Breakdown of the Reading Process:\n",
    "Opening the File: The h5py.File method opens the HDF5 file in read mode ('r').\n",
    "Reading the Data: The hf.keys() method retrieves the names of all datasets (keys) in the file, which you can loop over. Each dataset can be accessed and loaded into memory using hf[key][...].\n",
    "Storing in a Dictionary: Each dataset is stored in a dictionary called data, where the keys are the dataset names, and the values are the corresponding NumPy arrays.\n",
    "Accessing Host Galaxy Data:\n",
    "After reading the data, you can access the host galaxy data attributes using the keys you defined earlier. For example, you can retrieve fgas and other attributes as shown in the example.\n",
    "\n",
    "This way, you can easily save and later retrieve complex data structures, including multidimensional arrays and dictionaries, from HDF5 files.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "You said:\n",
    "file_name = pfolder + \"Illustris_Data/mbhb-evolution_no-ecc_lc-full-0.6.hdf5\"\n",
    "f1 = h5py.File(file_name,'r') \n",
    "\n",
    "print every key in f1\n",
    "ChatGPT said:\n",
    "ChatGPT\n",
    "To print every key in your HDF5 file (f1), you can loop through the keys like this:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "import h5py\n",
    "\n",
    "file_name = pfolder + \"Illustris_Data/mbhb-evolution_no-ecc_lc-full-0.6.hdf5\"\n",
    "f1 = h5py.File(file_name, 'r')\n",
    "\n",
    "# Print all keys in the HDF5 file\n",
    "for key in f1.keys():\n",
    "    print(key)\n",
    "This will print every top-level key in the HDF5 file. If the file contains groups (like folders in a directory), each group can have its own sub-keys, which you can access recursively.\n",
    "\n",
    "To print all keys, including those within groups, you can define a recursive function like this:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "def print_all_keys(hdf5_file, indent=0):\n",
    "    for key in hdf5_file.keys():\n",
    "        print(\" \" * indent + key)  # Indent for better readability\n",
    "        if isinstance(hdf5_file[key], h5py.Group):\n",
    "            print_all_keys(hdf5_file[key], indent + 2)  # Recursively print sub-keys\n",
    "\n",
    "# Example usage:\n",
    "print_all_keys(f1)\n",
    "This will print all keys in the file, including nested groups and dataset\n",
    "\n",
    "def print_hdf5_contents(hdf5_file, path='/', indent=0, limit=5):\n",
    "    '''Recursively print the contents of an HDF5 file, including datasets and groups.'''\n",
    "    for key in hdf5_file[path]:\n",
    "        item = hdf5_file[path + key]\n",
    "        \n",
    "        if isinstance(item, h5py.Group):\n",
    "            print(\" \" * indent + f\"Group: {path + key}/\")\n",
    "            # Recursively print the contents of this group\n",
    "            print_hdf5_contents(hdf5_file, path + key + '/', indent + 2)\n",
    "        elif isinstance(item, h5py.Dataset):\n",
    "            print(\" \" * indent + f\"Dataset: {path + key}\")\n",
    "            # Print the dataset content with a limit to avoid excessive output\n",
    "            data = item[...]\n",
    "            if data.size > limit:\n",
    "                print(\" \" * indent + f\" Data (showing first {limit} elements):\", data.flat[:limit])\n",
    "            else:\n",
    "                print(\" \" * indent + \" Data:\", data)\n",
    "\n",
    "# Example usage\n",
    "file_name = pfolder + \"Illustris_Data/mbhb-evolution_no-ecc_lc-full-0.6.hdf5\"\n",
    "f1 = h5py.File(file_name, 'r')\n",
    "\n",
    "# Print everything in the HDF5 file\n",
    "print_hdf5_contents(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the merger is the maximum value in each time evolution of the binary\n",
    "\n",
    "t_bin_merger = np.amax(t,1)/yrsec\n",
    "merger = [] #to indicate if the binary actually merges before hubble time\n",
    "z_binary = [] #convert the time of merger to redshifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for time in t_bin_merger:\n",
    "    if time >= age_of_the_universe:\n",
    "        #these black holes aren't merging\n",
    "        merger.append(\"No\")\n",
    "        z_binary.append(0)\n",
    "    else:\n",
    "        merger.append(\"Yes\")\n",
    "        z_binary.append(z_at_value(cosmo.age,(time/10**9)*u.Gyr,zmin=1e-9).value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([M1,M2,t_bin_merger,merger,z_binary])\n",
    "df = df.transpose()\n",
    "df.columns = ['M1', 'M2', 't_merger','Merger','Redshift']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M1</th>\n",
       "      <th>M2</th>\n",
       "      <th>t_merger</th>\n",
       "      <th>Merger</th>\n",
       "      <th>Redshift</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2261178.977273</td>\n",
       "      <td>1629005.681818</td>\n",
       "      <td>858774126.450153</td>\n",
       "      <td>Yes</td>\n",
       "      <td>6.459185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1614772.727273</td>\n",
       "      <td>1077349.431818</td>\n",
       "      <td>893400572.050411</td>\n",
       "      <td>Yes</td>\n",
       "      <td>6.265176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6554857.954545</td>\n",
       "      <td>1175723.011364</td>\n",
       "      <td>4120229485.36754</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1.587945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7039616.477273</td>\n",
       "      <td>6263607.954545</td>\n",
       "      <td>89337138643.547943</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11791392.045455</td>\n",
       "      <td>3306548.295455</td>\n",
       "      <td>14217143975.611784</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                M1              M2            t_merger Merger  Redshift\n",
       "0   2261178.977273  1629005.681818    858774126.450153    Yes  6.459185\n",
       "1   1614772.727273  1077349.431818    893400572.050411    Yes  6.265176\n",
       "2   6554857.954545  1175723.011364    4120229485.36754    Yes  1.587945\n",
       "3   7039616.477273  6263607.954545  89337138643.547943     No         0\n",
       "4  11791392.045455  3306548.295455  14217143975.611784     No         0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#binary ids\n",
    "mergers = np.load(pfolder+'Illustris_Data/ill-1_blackhole_mergers_fixed.npz')\n",
    "indexes = f1['evolution']['val_inds'][:]\n",
    "binary_ids = mergers['ids'][indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bhid1_bin_1</th>\n",
       "      <th>bhid2_bin_1</th>\n",
       "      <th>bhid1_bin_2</th>\n",
       "      <th>bhid2_bin_2</th>\n",
       "      <th>M1_bin_1</th>\n",
       "      <th>M2_bin_1</th>\n",
       "      <th>M1_bin_2</th>\n",
       "      <th>M2_bin_2</th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>tmerger</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>3.286581e+07</td>\n",
       "      <td>1.253781e+07</td>\n",
       "      <td>5.276829e+07</td>\n",
       "      <td>8.204738e+06</td>\n",
       "      <td>2.055999e+09</td>\n",
       "      <td>1.988185e+09</td>\n",
       "      <td>1.985633e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>3.402509e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>1.059404e+07</td>\n",
       "      <td>2.885124e+06</td>\n",
       "      <td>3.132749e+08</td>\n",
       "      <td>4.211761e+07</td>\n",
       "      <td>3.255727e+11</td>\n",
       "      <td>7.265512e+09</td>\n",
       "      <td>5.529691e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>1.655162e+07</td>\n",
       "      <td>6.537747e+06</td>\n",
       "      <td>7.257406e+07</td>\n",
       "      <td>4.653945e+07</td>\n",
       "      <td>5.274952e+10</td>\n",
       "      <td>1.032617e+10</td>\n",
       "      <td>3.025313e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>1.259004e+07</td>\n",
       "      <td>3.439988e+06</td>\n",
       "      <td>2.386990e+07</td>\n",
       "      <td>2.216544e+06</td>\n",
       "      <td>5.883422e+09</td>\n",
       "      <td>1.284130e+12</td>\n",
       "      <td>1.972738e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>1.325181e+08</td>\n",
       "      <td>1.408275e+07</td>\n",
       "      <td>4.551391e+08</td>\n",
       "      <td>1.002870e+06</td>\n",
       "      <td>1.206497e+10</td>\n",
       "      <td>7.389760e+09</td>\n",
       "      <td>4.814316e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    bhid1_bin_1   bhid2_bin_1   bhid1_bin_2   bhid2_bin_2      M1_bin_1  \\\n",
       "0  9.223372e+18  9.223372e+18  9.223372e+18  9.223372e+18  3.286581e+07   \n",
       "1  9.223372e+18  9.223372e+18  3.402509e+18  9.223372e+18  1.059404e+07   \n",
       "2  9.223372e+18  9.223372e+18  9.223372e+18  9.223372e+18  1.655162e+07   \n",
       "3  9.223372e+18  9.223372e+18  9.223372e+18  9.223372e+18  1.259004e+07   \n",
       "4  9.223372e+18  9.223372e+18  9.223372e+18  9.223372e+18  1.325181e+08   \n",
       "\n",
       "       M2_bin_1      M1_bin_2      M2_bin_2            t1            t2  \\\n",
       "0  1.253781e+07  5.276829e+07  8.204738e+06  2.055999e+09  1.988185e+09   \n",
       "1  2.885124e+06  3.132749e+08  4.211761e+07  3.255727e+11  7.265512e+09   \n",
       "2  6.537747e+06  7.257406e+07  4.653945e+07  5.274952e+10  1.032617e+10   \n",
       "3  3.439988e+06  2.386990e+07  2.216544e+06  5.883422e+09  1.284130e+12   \n",
       "4  1.408275e+07  4.551391e+08  1.002870e+06  1.206497e+10  7.389760e+09   \n",
       "\n",
       "        tmerger  \n",
       "0  1.985633e+09  \n",
       "1  5.529691e+09  \n",
       "2  3.025313e+09  \n",
       "3  1.972738e+09  \n",
       "4  4.814316e+09  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Triple_df = pd.read_csv(pfolder+\"Illustris_Data/triple_data_ill.csv\") #the triples data file from find_triples\n",
    "Triple_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13255795998.36437"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(Triple_df[\"M1_bin_1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bh1id1 = Triple_df[\"bhid1_bin_1\"].to_numpy()\n",
    "bh1id2 = Triple_df[\"bhid2_bin_1\"].to_numpy()\n",
    "bh2id1 = Triple_df[\"bhid1_bin_2\"].to_numpy()\n",
    "bh2id2 = Triple_df[\"bhid2_bin_2\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchID(id):\n",
    "\n",
    "    #to search if the binary ids belong to any triples, indicating a triple interaction\n",
    "\n",
    "    tf1 = id in bh1id1\n",
    "    tf2 = id in bh1id2\n",
    "    tf3 = id in bh2id1\n",
    "    tf4 = id in bh2id2\n",
    "\n",
    "    tf = tf1 + tf2 + tf3 + tf4\n",
    "\n",
    "    return tf \n",
    "\n",
    "merger_type = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "for i in range(Nbinary):\n",
    "\n",
    "    search1 = searchID(binary_ids[i][0])\n",
    "    search2 = searchID(binary_ids[i][1])\n",
    "\n",
    "    search = search1 + search2\n",
    "    if(search == 0):\n",
    "        merger_type.append(\"iso\")\n",
    "        \n",
    "    else:\n",
    "        merger_type.append(\"trip\")\n",
    "        indices.append(i)\n",
    "       \n",
    "\n",
    "df[\"Type\"] = merger_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Binaries: 9234\n",
      "Number of merged binaries: 2370\n",
      "Number of Isolated Binary mergers: 1929\n",
      "Number of Binaries with triple influence: 1401\n",
      "Number of Binary non mergers with triple influence: 960\n"
     ]
    }
   ],
   "source": [
    "\n",
    "binary_ids = np.array(binary_ids)\n",
    "df[\"BH1-ID\"] = binary_ids[:,0]\n",
    "df[\"BH2-ID\"] = binary_ids[:,1]\n",
    "\n",
    "print(\"Number of Binaries: %d\"%(len(df)))\n",
    "print(\"Number of merged binaries: %d\"%(len(df[df[\"Merger\"]==\"Yes\"])))\n",
    "print(\"Number of Isolated Binary mergers: %d\"%(len(df[(df[\"Merger\"]==\"Yes\") & (df[\"Type\"] == \"iso\")])))\n",
    "print(\"Number of Binaries with triple influence: %d\"%(len(df[(df[\"Type\"] == \"trip\")])))\n",
    "print(\"Number of Binary non mergers with triple influence: %d\"%(len(df[(df[\"Merger\"]==\"No\") & (df[\"Type\"] == \"trip\")])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"Data/binary-merger-data.csv\",index = False)\n",
    "print(\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1_list = []\n",
    "m1_id = []\n",
    "m2_list = []\n",
    "m2_id = []\n",
    "m3_list = []\n",
    "m3_id = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntriple = np.size(bh1id1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m m2_new \u001b[38;5;241m=\u001b[39m m2 \u001b[38;5;241m+\u001b[39m dM\u001b[38;5;241m*\u001b[39mqin_old\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (m1_new\u001b[38;5;241m>\u001b[39mm2_new):\n\u001b[0;32m---> 20\u001b[0m     \u001b[43mm1_list\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(m1_new)\n\u001b[1;32m     21\u001b[0m     m1_id\u001b[38;5;241m.\u001b[39mappend(Triple_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbhid1_bin_1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[i])\n\u001b[1;32m     22\u001b[0m     m2_list\u001b[38;5;241m.\u001b[39mappend(m2_new)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "ix_remove = []\n",
    "\n",
    "for i in range(Ntriple):\n",
    "\n",
    "    m1 = Triple_df[\"M1_bin_1\"].iloc[i]\n",
    "    m2 = Triple_df[\"M2_bin_1\"].iloc[i]\n",
    "    \n",
    "    qin_old = m2/m1\n",
    "\n",
    "    mA = Triple_df[\"M1_bin_2\"].iloc[i]\n",
    "    m3 = Triple_df[\"M2_bin_2\"].iloc[i]\n",
    "\n",
    "    dM = mA-(m1+m2)\n",
    "\n",
    "    if(dM>0):\n",
    "        m1_new = m1 + dM*(1-qin_old)\n",
    "        m2_new = m2 + dM*qin_old\n",
    "\n",
    "        if (m1_new>m2_new):\n",
    "            m1_list.append(m1_new)\n",
    "            m1_id.append(Triple_df[\"bhid1_bin_1\"].iloc[i])\n",
    "            m2_list.append(m2_new)\n",
    "            m2_id.append(Triple_df[\"bhid2_bin_1\"].iloc[i])\n",
    "        \n",
    "        else:\n",
    "            m1_list.append(m2_new)\n",
    "            m1_id.append(Triple_df[\"bhid2_bin_1\"].iloc[i])\n",
    "            m2_list.append(m1_new)\n",
    "            m2_id.append(Triple_df[\"bhid1_bin_1\"].iloc[i])\n",
    "        \n",
    "        m3_list.append(m3)\n",
    "        m3_id.append(Triple_df[\"bhid1_bin_2\"].iloc[i])\n",
    "    \n",
    "    else:\n",
    "        ix_remove.append(i)\n",
    "        \n",
    "\n",
    "m1_list = np.array(m1_list)\n",
    "m2_list = np.array(m2_list)\n",
    "m3_list = np.array(m3_list)\n",
    "\n",
    "m1_id = np.array(m1_id)\n",
    "m2_id = np.array(m2_id)\n",
    "m3_id = np.array(m3_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bhid1_bin_1</th>\n",
       "      <th>bhid2_bin_1</th>\n",
       "      <th>bhid1_bin_2</th>\n",
       "      <th>bhid2_bin_2</th>\n",
       "      <th>M1_bin_1</th>\n",
       "      <th>M2_bin_1</th>\n",
       "      <th>M1_bin_2</th>\n",
       "      <th>M2_bin_2</th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>tmerger</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>5.608023e+07</td>\n",
       "      <td>5.077300e+07</td>\n",
       "      <td>5.764539e+07</td>\n",
       "      <td>2.525074e+07</td>\n",
       "      <td>2.102321e+10</td>\n",
       "      <td>9.854869e+09</td>\n",
       "      <td>8.888190e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>1.467121e+08</td>\n",
       "      <td>1.188970e+08</td>\n",
       "      <td>1.659010e+08</td>\n",
       "      <td>1.266006e+08</td>\n",
       "      <td>1.947578e+10</td>\n",
       "      <td>1.201756e+10</td>\n",
       "      <td>1.052584e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.016216e+07</td>\n",
       "      <td>8.101857e+07</td>\n",
       "      <td>1.061407e+08</td>\n",
       "      <td>6.139939e+06</td>\n",
       "      <td>1.120306e+11</td>\n",
       "      <td>2.790481e+10</td>\n",
       "      <td>1.131227e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      bhid1_bin_1   bhid2_bin_1   bhid1_bin_2   bhid2_bin_2      M1_bin_1  \\\n",
       "497  9.223372e+18  9.223372e+18  9.223372e+18  9.223372e+18  5.608023e+07   \n",
       "498  9.223372e+18  9.223372e+18  9.223372e+18  9.223372e+18  1.467121e+08   \n",
       "500  9.223372e+18  9.223372e+18  9.223372e+18  9.223372e+18  9.016216e+07   \n",
       "\n",
       "         M2_bin_1      M1_bin_2      M2_bin_2            t1            t2  \\\n",
       "497  5.077300e+07  5.764539e+07  2.525074e+07  2.102321e+10  9.854869e+09   \n",
       "498  1.188970e+08  1.659010e+08  1.266006e+08  1.947578e+10  1.201756e+10   \n",
       "500  8.101857e+07  1.061407e+08  6.139939e+06  1.120306e+11  2.790481e+10   \n",
       "\n",
       "          tmerger  \n",
       "497  8.888190e+09  \n",
       "498  1.052584e+10  \n",
       "500  1.131227e+10  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Triple_df.iloc[ix_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bhid1_bin_1</th>\n",
       "      <th>bhid2_bin_1</th>\n",
       "      <th>bhid1_bin_2</th>\n",
       "      <th>bhid2_bin_2</th>\n",
       "      <th>M1_bin_1</th>\n",
       "      <th>M2_bin_1</th>\n",
       "      <th>M1_bin_2</th>\n",
       "      <th>M2_bin_2</th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>tmerger</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>3.286581e+07</td>\n",
       "      <td>1.253781e+07</td>\n",
       "      <td>5.276829e+07</td>\n",
       "      <td>8.204738e+06</td>\n",
       "      <td>2.055999e+09</td>\n",
       "      <td>1.988185e+09</td>\n",
       "      <td>1.985633e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>3.402509e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>1.059404e+07</td>\n",
       "      <td>2.885124e+06</td>\n",
       "      <td>3.132749e+08</td>\n",
       "      <td>4.211761e+07</td>\n",
       "      <td>3.255727e+11</td>\n",
       "      <td>7.265512e+09</td>\n",
       "      <td>5.529691e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>1.655162e+07</td>\n",
       "      <td>6.537747e+06</td>\n",
       "      <td>7.257406e+07</td>\n",
       "      <td>4.653945e+07</td>\n",
       "      <td>5.274952e+10</td>\n",
       "      <td>1.032617e+10</td>\n",
       "      <td>3.025313e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>1.259004e+07</td>\n",
       "      <td>3.439988e+06</td>\n",
       "      <td>2.386990e+07</td>\n",
       "      <td>2.216544e+06</td>\n",
       "      <td>5.883422e+09</td>\n",
       "      <td>1.284130e+12</td>\n",
       "      <td>1.972738e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>1.325181e+08</td>\n",
       "      <td>1.408275e+07</td>\n",
       "      <td>4.551391e+08</td>\n",
       "      <td>1.002870e+06</td>\n",
       "      <td>1.206497e+10</td>\n",
       "      <td>7.389760e+09</td>\n",
       "      <td>4.814316e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>5.557129e+07</td>\n",
       "      <td>8.642577e+06</td>\n",
       "      <td>1.019236e+08</td>\n",
       "      <td>6.895766e+07</td>\n",
       "      <td>3.514319e+10</td>\n",
       "      <td>1.323119e+10</td>\n",
       "      <td>1.241580e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>2.426127e+08</td>\n",
       "      <td>2.139748e+07</td>\n",
       "      <td>2.724291e+08</td>\n",
       "      <td>1.692168e+06</td>\n",
       "      <td>1.468992e+10</td>\n",
       "      <td>1.466670e+10</td>\n",
       "      <td>1.354208e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>1.325318e+07</td>\n",
       "      <td>8.303175e+06</td>\n",
       "      <td>2.158876e+07</td>\n",
       "      <td>6.823273e+06</td>\n",
       "      <td>1.927055e+11</td>\n",
       "      <td>1.358702e+10</td>\n",
       "      <td>1.311330e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>4.575134e+07</td>\n",
       "      <td>9.468752e+06</td>\n",
       "      <td>5.555056e+07</td>\n",
       "      <td>5.338969e+07</td>\n",
       "      <td>4.607816e+10</td>\n",
       "      <td>1.945157e+10</td>\n",
       "      <td>1.355722e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>5.098771e+07</td>\n",
       "      <td>2.248239e+06</td>\n",
       "      <td>5.555056e+07</td>\n",
       "      <td>5.338969e+07</td>\n",
       "      <td>1.499998e+11</td>\n",
       "      <td>1.945157e+10</td>\n",
       "      <td>1.373661e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>531 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      bhid1_bin_1   bhid2_bin_1   bhid1_bin_2   bhid2_bin_2      M1_bin_1  \\\n",
       "0    9.223372e+18  9.223372e+18  9.223372e+18  9.223372e+18  3.286581e+07   \n",
       "1    9.223372e+18  9.223372e+18  3.402509e+18  9.223372e+18  1.059404e+07   \n",
       "2    9.223372e+18  9.223372e+18  9.223372e+18  9.223372e+18  1.655162e+07   \n",
       "3    9.223372e+18  9.223372e+18  9.223372e+18  9.223372e+18  1.259004e+07   \n",
       "4    9.223372e+18  9.223372e+18  9.223372e+18  9.223372e+18  1.325181e+08   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "526  9.223372e+18  9.223372e+18  9.223372e+18  9.223372e+18  5.557129e+07   \n",
       "527  9.223372e+18  9.223372e+18  9.223372e+18  9.223372e+18  2.426127e+08   \n",
       "528  9.223372e+18  9.223372e+18  9.223372e+18  9.223372e+18  1.325318e+07   \n",
       "529  9.223372e+18  9.223372e+18  9.223372e+18  9.223372e+18  4.575134e+07   \n",
       "530  9.223372e+18  9.223372e+18  9.223372e+18  9.223372e+18  5.098771e+07   \n",
       "\n",
       "         M2_bin_1      M1_bin_2      M2_bin_2            t1            t2  \\\n",
       "0    1.253781e+07  5.276829e+07  8.204738e+06  2.055999e+09  1.988185e+09   \n",
       "1    2.885124e+06  3.132749e+08  4.211761e+07  3.255727e+11  7.265512e+09   \n",
       "2    6.537747e+06  7.257406e+07  4.653945e+07  5.274952e+10  1.032617e+10   \n",
       "3    3.439988e+06  2.386990e+07  2.216544e+06  5.883422e+09  1.284130e+12   \n",
       "4    1.408275e+07  4.551391e+08  1.002870e+06  1.206497e+10  7.389760e+09   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "526  8.642577e+06  1.019236e+08  6.895766e+07  3.514319e+10  1.323119e+10   \n",
       "527  2.139748e+07  2.724291e+08  1.692168e+06  1.468992e+10  1.466670e+10   \n",
       "528  8.303175e+06  2.158876e+07  6.823273e+06  1.927055e+11  1.358702e+10   \n",
       "529  9.468752e+06  5.555056e+07  5.338969e+07  4.607816e+10  1.945157e+10   \n",
       "530  2.248239e+06  5.555056e+07  5.338969e+07  1.499998e+11  1.945157e+10   \n",
       "\n",
       "          tmerger  \n",
       "0    1.985633e+09  \n",
       "1    5.529691e+09  \n",
       "2    3.025313e+09  \n",
       "3    1.972738e+09  \n",
       "4    4.814316e+09  \n",
       "..            ...  \n",
       "526  1.241580e+10  \n",
       "527  1.354208e+10  \n",
       "528  1.311330e+10  \n",
       "529  1.355722e+10  \n",
       "530  1.373661e+10  \n",
       "\n",
       "[531 rows x 11 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Triple_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1 = m1_list\n",
    "qin = m2_list/m1_list\n",
    "qout = m3_list/(m1_list + m2_list)\n",
    "\n",
    "t_triple = Triple_df[\"tmerger\"].to_numpy()\n",
    "z_triple = z_at_value(cosmo.age,(t_triple/10**9)*u.Gyr,zmin=1e-10)\n",
    "z_triple = np.array(z_triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_triple = np.delete(t_triple,ix_remove)\n",
    "z_triple = np.delete(z_triple,ix_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = ['M1','M2','M3','qin','qout','t_triple','z_triple','M1_ID','M2_ID','M3_ID']\n",
    "Triple_df_mod = pd.DataFrame({'M1': M1,'M2':m2_list,'M3':m3_list,'qin': qin, 'qout': qout, 't_triple' : t_triple, 'z_triple' : z_triple,'M1_ID':m1_id,'M2_ID':m2_id,'M3_ID':m3_id},columns=column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>M1</th>\n",
       "      <th>M2</th>\n",
       "      <th>M3</th>\n",
       "      <th>qin</th>\n",
       "      <th>qout</th>\n",
       "      <th>t_triple</th>\n",
       "      <th>z_triple</th>\n",
       "      <th>M1_ID</th>\n",
       "      <th>M2_ID</th>\n",
       "      <th>M3_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.742097e+07</td>\n",
       "      <td>1.534732e+07</td>\n",
       "      <td>8.204738e+06</td>\n",
       "      <td>0.410126</td>\n",
       "      <td>0.155486</td>\n",
       "      <td>1.985633e+09</td>\n",
       "      <td>3.257459</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.287451e+08</td>\n",
       "      <td>8.452986e+07</td>\n",
       "      <td>4.211761e+07</td>\n",
       "      <td>0.369537</td>\n",
       "      <td>0.134443</td>\n",
       "      <td>5.529691e+09</td>\n",
       "      <td>1.101818</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>3.402509e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.649029e+07</td>\n",
       "      <td>2.608377e+07</td>\n",
       "      <td>4.653945e+07</td>\n",
       "      <td>0.561058</td>\n",
       "      <td>0.641268</td>\n",
       "      <td>3.025313e+09</td>\n",
       "      <td>2.201414</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.828782e+07</td>\n",
       "      <td>5.582083e+06</td>\n",
       "      <td>2.216544e+06</td>\n",
       "      <td>0.305235</td>\n",
       "      <td>0.092859</td>\n",
       "      <td>1.972738e+09</td>\n",
       "      <td>3.276165</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.082678e+08</td>\n",
       "      <td>4.687125e+07</td>\n",
       "      <td>1.002870e+06</td>\n",
       "      <td>0.114805</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>4.814316e+09</td>\n",
       "      <td>1.320180</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>8.741631e+07</td>\n",
       "      <td>1.450728e+07</td>\n",
       "      <td>6.895766e+07</td>\n",
       "      <td>0.165956</td>\n",
       "      <td>0.676562</td>\n",
       "      <td>1.241580e+10</td>\n",
       "      <td>0.103086</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>2.502891e+08</td>\n",
       "      <td>2.214000e+07</td>\n",
       "      <td>1.692168e+06</td>\n",
       "      <td>0.088458</td>\n",
       "      <td>0.006211</td>\n",
       "      <td>1.354208e+10</td>\n",
       "      <td>0.016267</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>1.326529e+07</td>\n",
       "      <td>8.323477e+06</td>\n",
       "      <td>6.823273e+06</td>\n",
       "      <td>0.627463</td>\n",
       "      <td>0.316057</td>\n",
       "      <td>1.311330e+10</td>\n",
       "      <td>0.048085</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>4.601341e+07</td>\n",
       "      <td>9.537147e+06</td>\n",
       "      <td>5.338969e+07</td>\n",
       "      <td>0.207269</td>\n",
       "      <td>0.961101</td>\n",
       "      <td>1.355722e+10</td>\n",
       "      <td>0.015170</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>5.320026e+07</td>\n",
       "      <td>2.350299e+06</td>\n",
       "      <td>5.338969e+07</td>\n",
       "      <td>0.044178</td>\n",
       "      <td>0.961101</td>\n",
       "      <td>1.373661e+10</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "      <td>9.223372e+18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>528 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               M1            M2            M3       qin      qout  \\\n",
       "0    3.742097e+07  1.534732e+07  8.204738e+06  0.410126  0.155486   \n",
       "1    2.287451e+08  8.452986e+07  4.211761e+07  0.369537  0.134443   \n",
       "2    4.649029e+07  2.608377e+07  4.653945e+07  0.561058  0.641268   \n",
       "3    1.828782e+07  5.582083e+06  2.216544e+06  0.305235  0.092859   \n",
       "4    4.082678e+08  4.687125e+07  1.002870e+06  0.114805  0.002203   \n",
       "..            ...           ...           ...       ...       ...   \n",
       "523  8.741631e+07  1.450728e+07  6.895766e+07  0.165956  0.676562   \n",
       "524  2.502891e+08  2.214000e+07  1.692168e+06  0.088458  0.006211   \n",
       "525  1.326529e+07  8.323477e+06  6.823273e+06  0.627463  0.316057   \n",
       "526  4.601341e+07  9.537147e+06  5.338969e+07  0.207269  0.961101   \n",
       "527  5.320026e+07  2.350299e+06  5.338969e+07  0.044178  0.961101   \n",
       "\n",
       "         t_triple  z_triple         M1_ID         M2_ID         M3_ID  \n",
       "0    1.985633e+09  3.257459  9.223372e+18  9.223372e+18  9.223372e+18  \n",
       "1    5.529691e+09  1.101818  9.223372e+18  9.223372e+18  3.402509e+18  \n",
       "2    3.025313e+09  2.201414  9.223372e+18  9.223372e+18  9.223372e+18  \n",
       "3    1.972738e+09  3.276165  9.223372e+18  9.223372e+18  9.223372e+18  \n",
       "4    4.814316e+09  1.320180  9.223372e+18  9.223372e+18  9.223372e+18  \n",
       "..            ...       ...           ...           ...           ...  \n",
       "523  1.241580e+10  0.103086  9.223372e+18  9.223372e+18  9.223372e+18  \n",
       "524  1.354208e+10  0.016267  9.223372e+18  9.223372e+18  9.223372e+18  \n",
       "525  1.311330e+10  0.048085  9.223372e+18  9.223372e+18  9.223372e+18  \n",
       "526  1.355722e+10  0.015170  9.223372e+18  9.223372e+18  9.223372e+18  \n",
       "527  1.373661e+10  0.002293  9.223372e+18  9.223372e+18  9.223372e+18  \n",
       "\n",
       "[528 rows x 10 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Triple_df_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "Triple_df_mod.to_csv('Data/Triple-mass-redshift.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
